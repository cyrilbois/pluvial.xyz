---
title: Philosophy, Rationality
---

# Philosophy, Rationality

[The Last Question](https://users.ece.cmu.edu/~gamvrosi/thelastq.html)

[sam lessin üè¥‚Äç‚ò†Ô∏è on Twitter: "FB‚Äôs has an interface problem, not an algorithm problem‚Ä¶ because people won‚Äôt affirmatively click on the things they actually deep down want to watch https://t.co/QfEiaOf13j" / Twitter](https://twitter.com/lessin/status/1556986127785115648)

[(4) sam lessin üè¥‚Äç‚ò†Ô∏è on Twitter: "The coming fall of the Kardashians in context of how entertainment is evolving... (aka why they are so pissed about tiktok) https://t.co/wtYrvxbS35" / Twitter](https://twitter.com/lessin/status/1551931628305502208)

[The End of Social Media - Michael Mignano | Medium](https://mignano.medium.com/the-end-of-social-media-a88ffed21f86)

[Shameless Samsung ‚Äì Stratechery by Ben Thompson](https://stratechery.com/2013/shameless-samsung/)

[Messaging: Mobile‚Äôs Killer App ‚Äì Stratechery by Ben Thompson](https://stratechery.com/2014/messaging-mobiles-killer-app/)

[Aggregation Theory ‚Äì Stratechery by Ben Thompson](https://stratechery.com/2015/aggregation-theory/)

[Snapchat‚Äôs Ladder ‚Äì Stratechery by Ben Thompson](https://stratechery.com/2016/snapchats-ladder/)

[Facebook, Phones, and Phonebooks ‚Äì Stratechery by Ben Thompson](https://stratechery.com/2016/what-facebook-is-and-isnt/)

[Goodbye Gatekeepers ‚Äì Stratechery by Ben Thompson](https://stratechery.com/2017/goodbye-gatekeepers/)

[The Internet and the Third Estate ‚Äì Stratechery by Ben Thompson](https://stratechery.com/2019/the-internet-and-the-third-estate/)

[The TikTok War ‚Äì Stratechery by Ben Thompson](https://stratechery.com/2020/the-tiktok-war/)

[Mistakes and Memes ‚Äì Stratechery by Ben Thompson](https://stratechery.com/2021/mistakes-and-memes/)

[Instagram‚Äôs Evolution ‚Äì Stratechery by Ben Thompson](https://stratechery.com/2021/instagrams-evolution/)

[Metaverses ‚Äì Stratechery by Ben Thompson](https://stratechery.com/2021/metaverses/)

[Three Trends Follow-Up, The Question of ‚ÄúCool‚Äù, TikTok and the Sinicization of the Internet ‚Äì Stratechery by Ben Thompson](https://stratechery.com/2022/three-trends-follow-up-the-question-of-cool-tiktok-and-the-sinicization-of-the-internet/)

[Stratechery by Ben Thompson ‚Äì On the business, strategy, and impact of technology.](https://stratechery.com/)

[Asymptotic safety in quantum gravity - Wikipedia](https://en.wikipedia.org/wiki/Asymptotic_safety_in_quantum_gravity?useskin=vector)

[Physics applications of asymptotically safe gravity - Wikipedia](https://en.wikipedia.org/wiki/Physics_applications_of_asymptotically_safe_gravity?useskin=vector)

[Induced gravity - Wikipedia](https://en.wikipedia.org/wiki/Induced_gravity?useskin=vector)

[Quantum gravity - Wikipedia](https://en.wikipedia.org/wiki/Quantum_gravity?useskin=vector)

[Modified Newtonian dynamics - Wikipedia](https://en.wikipedia.org/wiki/Modified_Newtonian_dynamics?useskin=vector)

[Causal sets - Wikipedia](https://en.wikipedia.org/wiki/Causal_sets?useskin=vector)

[Twistor theory - Wikipedia](https://en.wikipedia.org/wiki/Twistor_theory?useskin=vector)

[Appromoximate](https://appromoximate.com/)

[(4) Venkatesh Rao ‚òÄÔ∏è (@vgr) / Twitter](https://twitter.com/vgr)

[ribbonfarm ‚Äì constructions in magical thinking](https://www.ribbonfarm.com/)

[Ribbonfarm Studio | Venkatesh Rao | Substack](https://studio.ribbonfarm.com/)

[AMMDI: Protocol Thinking](http://hyperphor.com/ammdi/Protocol-Thinking)

[The Unreasonable Sufficiency of Protocols - Summer of Protocols](https://venkatesh-rao.gitbook.io/summer-of-protocols/)

[Computational Law, Symbolic Discourse and the AI Constitution‚ÄîStephen Wolfram Writings](https://writings.stephenwolfram.com/2016/10/computational-law-symbolic-discourse-and-the-ai-constitution/)

[Multicomputation: A Fourth Paradigm for Theoretical Science‚ÄîStephen Wolfram Writings](https://writings.stephenwolfram.com/2021/09/multicomputation-a-fourth-paradigm-for-theoretical-science/)

[The Concept of the Ruliad‚ÄîStephen Wolfram Writings](https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/)

[Galactica: an AI trained on humanity's scientific knowledge (by Meta) | Hacker News](https://news.ycombinator.com/item?id=33611265)

[Chief scientist of major corporation can‚Äôt handle criticism of the work he hypes | Hacker News](https://news.ycombinator.com/item?id=33719763)

[Writing/the_double_edged_sword_of_AI.md at main ¬∑ Liu-Eroteme/Writing ¬∑ GitHub](https://github.com/Liu-Eroteme/Writing/blob/main/the_double_edged_sword_of_AI.md)

[The End of Programming | January 2023 | Communications of the ACM](https://cacm.acm.org/magazines/2023/1/267976-the-end-of-programming/fulltext)

[Large Language Model: world models or surface statistics?](https://thegradient.pub/othello/)

[Scoring forecasts from the 2016 ‚ÄúExpert Survey on Progress in AI‚Äù - EA Forum](https://forum.effectivealtruism.org/posts/tCkBsT6cAw6LEKAbm/scoring-forecasts-from-the-2016-expert-survey-on-progress-in)

[More Is Different | Science](https://www.science.org/doi/10.1126/science.177.4047.393?ref=bounded-regret.ghost.io)

[Transcript: Ezra Klein Interviews Gary Marcus - The New York Times](https://archive.is/zyEP1)

[What does it mean when an AI fails? A Reply to SlateStarCodex‚Äôs riff on Gary Marcus](https://garymarcus.substack.com/p/what-does-it-mean-when-an-ai-fails)

[The Road to AI We Can Trust | Gary Marcus | Substack](https://garymarcus.substack.com/)

[A reply to Michael Huemer on AI - Matthew Barnett‚Äôs Blog](https://matthewbarnett.substack.com/p/a-reply-to-michael-huemer-on-ai)

[Matthew Barnett‚Äôs Blog | Substack](https://matthewbarnett.substack.com/)

[Erich Grunewald's Blog](https://www.erichgrunewald.com/)

[Meditations On Moloch | Slate Star Codex](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/)

[Raikoth: Laws, Language, and Society | Slate Star Codex](https://slatestarcodex.com/2013/05/06/raikoth-laws-language-and-society/)

[Searching For One-Sided Tradeoffs | Slate Star Codex](https://slatestarcodex.com/2014/03/01/searching-for-one-sided-tradeoffs/)

[Archipelago and Atomic Communitarianism | Slate Star Codex](https://slatestarcodex.com/2014/06/07/archipelago-and-atomic-communitarianism/)

[Poor Folks Do Smile‚Ä¶For Now | Slate Star Codex](https://slatestarcodex.com/2013/04/06/poor-folks-do-smile-for-now/)

[GPT-2 As Step Toward General Intelligence | Slate Star Codex](https://slatestarcodex.com/2019/02/19/gpt-2-as-step-toward-general-intelligence/)

[The Book of Sand - Wikipedia](https://en.wikipedia.org/wiki/The_Book_of_Sand?useskin=vector)

[The Aleph. Borgean fantastic hyperreality‚Ä¶ | by The Sandbook | Medium](https://medium.com/@die.zaubernacht/the-aleph-3435f52ac297)

[Mechanical Sympathy: Understanding the Hardware Makes You a Better Developer - DZone](https://dzone.com/articles/mechanical-sympathy)

[Evidential decision theory - Wikipedia](https://en.wikipedia.org/wiki/Evidential_decision_theory?useskin=vector)

[Now you can (try to) serve five terabytes, too](https://rachelbythebay.com/w/2021/10/30/5tb/)

[Rekt - Value DeFi - REKT 2](https://rekt.news/value-rekt2/)

[Crypto Firm Nomad Loses Nearly $200 Million in Bridge Hack - Bloomberg](https://www.bloomberg.com/news/articles/2022-08-02/crypto-bridge-nomad-drained-of-nearly-200-million-in-exploit#xj4y7vzkg)

[Federated learning - Wikipedia](https://en.wikipedia.org/wiki/Federated_learning?useskin=vector)

[The Dirty Pipe Vulnerability ‚Äî The Dirty Pipe Vulnerability documentation](https://dirtypipe.cm4all.com/)

[CVE-2022-21449: Psychic Signatures in Java ‚Äì Neil Madden](https://neilmadden.blog/2022/04/19/psychic-signatures-in-java/)

[Thomas H. Ptacek (@tqbf): "It is nevertheless funny that there is a Wycheproof test for this bug (of course there is, it‚Äôs the most basic implementation check in ECDSA) and nobody bothered to run it against one of the most important ECDSA‚Äôs until now." | nitter](https://nitter.moomoo.me/tqbf/status/1516577012361662466)

[CVE-2022-34718 - Security Update Guide - Microsoft - Windows TCP/IP Remote Code Execution Vulnerability](https://msrc.microsoft.com/update-guide/en-US/vulnerability/CVE-2022-34718)

[Deconstructing Deathism - Answering Objections to Immortality - ImmortalLife.net](https://gwern.net/doc/philosophy/mind/2004-perry.html)

[anishmaxxing (@thiteanish): "@ggerganov's LLaMA works on a Pixel 6! LLaMAs been waiting for this, and so have I" | nitter](https://nitter.moomoo.me/thiteanish/status/1635188333705043969)

[Community Alert: Ronin Validators Compromised](https://roninblockchain.substack.com/p/community-alert-ronin-validators)

[Honey, I hacked the Empathy Machine!](https://bullfrogreview.substack.com/p/honey-i-hacked-the-empathy-machine)

[Brandolini's law - Wikipedia](https://en.wikipedia.org/wiki/Brandolini's_law?useskin=vector)

[Apple, Meta Gave User Data to Hackers With Forged Legal Requests (AAPL, FB) - Bloomberg](https://archive.ph/pY1dJ)

[Hackers Gaining Power of Subpoena Via Fake ‚ÄúEmergency Data Requests‚Äù ‚Äì Krebs on Security](https://krebsonsecurity.com/2022/03/hackers-gaining-power-of-subpoena-via-fake-emergency-data-requests/)

[Mirai (malware) - Wikipedia](https://en.wikipedia.org/wiki/Mirai_(malware)?useskin=vector)

[Uber apparently hacked by teen, employees thought it was a joke - The Verge](https://www.theverge.com/2022/9/16/23356213/uber-hack-teen-slack-google-cloud-credentials-powershell)

[2020 Twitter account hijacking - Wikipedia](https://en.wikipedia.org/wiki/2020_Twitter_account_hijacking?useskin=vector)

[The Billion Dollar AI Problem That Just Keeps Scaling](https://www.nextplatform.com/2021/02/11/the-billion-dollar-ai-problem-that-just-keeps-scaling/)

[1.1 - Fermi estimate of future training runs](https://www.danieldewey.net/risk/estimates.html)

[Factored Cognition - AI Alignment Forum](https://www.alignmentforum.org/tag/factored-cognition)

[The Toxoplasma Of Rage | Slate Star Codex](https://slatestarcodex.com/2014/12/17/the-toxoplasma-of-rage/)

[xkcd: Duty Calls](https://xkcd.com/386/)

[Sort By Controversial | Slate Star Codex](https://slatestarcodex.com/2018/10/30/sort-by-controversial/)

[CoreWeave ‚Äî The GPU Cloud](https://www.coreweave.com/)

[Target Hackers Broke in Via HVAC Company ‚Äì Krebs on Security](https://krebsonsecurity.com/2014/02/target-hackers-broke-in-via-hvac-company/)

[Chinese Spies Hacked a Livestock App to Breach US State Networks | WIRED](https://www.wired.com/story/china-apt41-hacking-usaherds-log4j/)

[harry,whg.eth ü¶äüíô (@sniko_): "Supply chain attacks" | nitter](https://nitter.moomoo.me/sniko_/status/1523984725840478208)

[China Has Already Reached Exascale ‚Äì On Two Separate Systems](https://gwern.net/doc/www/www.nextplatform.com/53b025b97f3c269f6ef945411090fbfdbe332593.html)

[John Carmack (@ID_AA_Carmack): "today, but if challenges demanded it, there is a world with a zetta scale, tightly integrated, low latency matrix dissipating a gigawatt in a swimming pool of circulating fluorinert." | nitter](https://nitter.moomoo.me/ID_AA_Carmack/status/1300280139717189640)

[NYU Accidentally Exposed Military Code-breaking Computer Project to Entire Internet](https://theintercept.com/2017/05/11/nyu-accidentally-exposed-military-code-breaking-computer-project-to-entire-internet/)

[Flatiron Institute - Wikipedia](https://en.wikipedia.org/wiki/Flatiron_Institute?useskin=vector)

[Is Programmable Overhead Worth The Cost?](https://semiengineering.com/is-programmable-overhead-worth-the-cost/)

[Cerebras - Wikipedia](https://en.wikipedia.org/wiki/Cerebras?useskin=vector#Technology)

[Extrapolating GPT-N performance - AI Alignment Forum](https://www.alignmentforum.org/posts/k2SNji3jXaLGhBeYP/extrapolating-gpt-n-performance)

[Computer Scientists Achieve ‚ÄòCrown Jewel‚Äô of Cryptography | Quanta Magazine](https://gwern.net/doc/www/www.quantamagazine.org/195c46c1aa527786f36fe3e0cd7a9be418c54c78.html)

[Rapid Locomotion via Reinforcement Learning](https://sites.google.com/view/model-free-speed/)

[Cerebro-cerebellar networks facilitate learning through feedback decoupling | bioRxiv](https://www.biorxiv.org/content/10.1101/2022.01.28.477827v1.full)

[Experience curve effects - Wikipedia](https://en.wikipedia.org/wiki/Experience_curve_effects?useskin=vector)

[Thread: Differentiable Self-organizing Systems](https://distill.pub/2020/selforg/)

[Self-Organising Textures](https://distill.pub/selforg/2021/textures/)

[Growing Neural Cellular Automata](https://distill.pub/2020/growing-ca/#google)

[Adversarial Reprogramming of Neural Cellular Automata](https://distill.pub/selforg/2021/adversarial/)

[The Future of Artificial Intelligence is Self-Organizing and Self-Assembling ‚Äì Sebastian Risi](https://sebastianrisi.com/self_assembling_ai/)

[Bioelectric Networks: Taming the Collective Intelligence of Cells for Regenerative Medicine - Foresight Institute](https://foresight.org/summary/bioelectric-networks-taming-the-collective-intelligence-of-cells-for-regenerative-medicine/)

[On Having No Head: Cognition throughout Biological Systems - PMC](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4914563/)

[Flying Fish and Aquarium Pets Yield Secrets of Evolution | Quanta Magazine](https://www.quantamagazine.org/flying-fish-and-aquarium-pets-yield-secrets-of-evolution-20220105/)

[Synthetic living machines: A new window on life: iScience](https://www.cell.com/iscience/fulltext/S2589-0042(21)00473-9)

[Fundamental behaviors emerge from simulations of a living minimal cell: Cell](https://www.cell.com/cell/fulltext/S0092-8674(21)01488-4)

[An Account of Electricity and the Body, Reviewed | The New Yorker](https://www.newyorker.com/magazine/2021/12/06/understanding-the-body-electric)

[Is Bioelectricity the Key to Limb Regeneration? | The New Yorker](https://www.newyorker.com/magazine/2021/05/10/persuading-the-body-to-regenerate-its-limbs)

[‚ÄòAmazing science‚Äô: researchers find xenobots can give rise to offspring | Science | The Guardian](https://www.theguardian.com/science/2021/nov/29/amazing-science-researchers-find-xenobots-can-give-rise-to-offspring)

[A synthetic protein-level neural network in mammalian cells | bioRxiv](https://www.biorxiv.org/content/10.1101/2022.07.10.499405v1.full)

[Cells Form Into ‚ÄòXenobots‚Äô on Their Own | Quanta Magazine](https://gwern.net/doc/www/www.quantamagazine.org/7817dd693b80c41bf7d3cbc0d530b87435175546.html)

[9 Missile Commanders Fired, Others Disciplined In Air Force Scandal : The Two-Way : NPR](https://www.npr.org/sections/thetwo-way/2014/03/27/295314331/9-missile-commanders-fired-others-disciplined-in-air-force-scandal)

[Security troops on US nuclear missile base took LSD | AP News](https://apnews.com/article/politics-ap-top-news-wyoming-north-america-social-media-98f903367b50404cb3c9695bcabefa5a)

[Joan Rohlfing on how to avoid catastrophic nuclear blunders - 80,000 Hours](https://80000hours.org/podcast/episodes/joan-rohlfing-avoiding-catastrophic-nuclear-blunders/#the-interaction-between-nuclear-weapons-and-cybersecurity-011018)

[The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)

[[D] Instances of (non-log) capability spikes or emergent behaviors in NNs? : mlscaling](https://old.reddit.com/r/mlscaling/comments/sjzvl0/d_instances_of_nonlog_capability_spikes_or/)

[In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#anthropic)

[SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient | OpenReview](https://openreview.net/forum?id=U1edbV4kNu_)

[Robert Oppenheimer - Wikiquote](https://en.wikiquote.org/wiki/Robert_Oppenheimer#Quotes)

[DeepMind and Google: the battle to control artificial intelligence | The Economist](https://www.economist.com/1843/2019/03/01/deepmind-and-google-the-battle-to-control-artificial-intelligence)

[Boosting Search Engines with Interactive Agents | OpenReview](https://openreview.net/forum?id=0ZbPmmB61g#google)

[Learning Robust Real-Time Cultural Transmission without Human Data](https://www.deepmind.com/publications/learning-robust-real-time-cultural-transmission-without-human-data)

[What Are Bayesian Neural Network Posteriors Really Like?](https://proceedings.mlr.press/v139/izmailov21a.html)

[Recurrent Experience Replay in Distributed Reinforcement Learning | OpenReview](https://openreview.net/forum?id=r1lyTjAqYX#deepmind)

[Microsoft researchers win ImageNet computer vision challenge - The AI Blog](https://blogs.microsoft.com/ai/microsoft-researchers-win-imagenet-computer-vision-challenge/)

[A Recipe for Training Neural Networks](https://karpathy.github.io/2019/04/25/recipe/)

[Solving (some) formal math olympiad problems](https://openai.com/research/formal-math)

[OpenAI Five defeats Dota 2 world champions](https://openai.com/research/openai-five-defeats-dota-2-world-champions)

[AI and compute](https://openai.com/research/ai-and-compute)

[AI and efficiency](https://openai.com/research/ai-and-efficiency)

[Scaling Laws for Language Transfer Learning](https://gwern.net/doc/www/christina.kim/44e9f6a746fe2e8c63736ca326d4cc5e19b3771e.html#openai)

[DALL¬∑E: Creating images from text](https://openai.com/research/dall-e)

[Fine-tuning GPT-2 from human preferences](https://openai.com/research/fine-tuning-gpt-2#bugscanoptimizeforbadbehavior)

[DeepSpeed: Accelerating large-scale model inference and training via system optimizations and compression - Microsoft Research](https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/)

[ZeRO-Infinity and DeepSpeed: Unlocking unprecedented model scale for deep learning training - Microsoft Research](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/)

[Effect of scale on catastrophic forgetting in neural networks | OpenReview](https://openreview.net/forum?id=GhVS8_yPeEa)

[v2appf](https://history.nasa.gov/rogersrep/v2appf.htm)

[Reward is enough - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0004370221000862#deepmind)

[Rip van Winkle's Razor, a Simple New Estimate for Adaptive Data Analysis ‚Äì Off the convex path](http://www.offconvex.org/2021/04/07/ripvanwinkle/)

[The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)

[The neural architecture of language: Integrative modeling converges on predictive processing | bioRxiv](https://www.biorxiv.org/content/10.1101/2020.06.26.174482v3.full)

[GPT-3 Samples - JustPaste.it](https://justpaste.it/7eovk)

[experience curves tag ¬∑ Gwern.net](https://gwern.net/doc/economics/experience-curve/index)

[Why Tool AIs Want to Be Agent AIs ¬∑ Gwern.net](https://gwern.net/tool-ai)

[preference learning tag ¬∑ Gwern.net](https://gwern.net/doc/reinforcement-learning/preference-learning/index#decisiontransformer-blog-section)

[Codex tag ¬∑ Gwern.net](https://gwern.net/doc/ai/nn/transformer/gpt/codex/index)

[MuZero tag ¬∑ Gwern.net](https://gwern.net/doc/reinforcement-learning/model/muzero/index)

[Fully-Connected Neural Nets ¬∑ Gwern.net](https://gwern.net/note/fc)

[Surprisingly Turing-Complete ¬∑ Gwern.net](https://gwern.net/turing-complete)

[How Many Computers Are In Your Computer? ¬∑ Gwern.net](https://gwern.net/computers)

[NN sparsity tag ¬∑ Gwern.net](https://gwern.net/doc/ai/nn/sparsity/index)

[Computer Optimization: Your Computer Is Faster Than You Think ¬∑ Gwern.net](https://gwern.net/note/faster)

[economics/automation tag ¬∑ Gwern.net](https://gwern.net/doc/economics/automation/index)

[end-to-end tag ¬∑ Gwern.net](https://gwern.net/doc/cs/end-to-end-principle/index)

[Complexity no Bar to AI ¬∑ Gwern.net](https://gwern.net/complexity)

[cognitive biases/illusion-of-depth tag ¬∑ Gwern.net](https://gwern.net/doc/psychology/cognitive-bias/illusion-of-depth/index)

[inner monologue (AI) tag ¬∑ Gwern.net](https://gwern.net/doc/ai/nn/transformer/gpt/inner-monologue/index)

[preference learning tag ¬∑ Gwern.net](https://gwern.net/doc/reinforcement-learning/preference-learning/index)

[meta-learning tag ¬∑ Gwern.net](https://gwern.net/doc/reinforcement-learning/meta-learning/index)

[Technology Forecasting: The Garden of Forking Paths ¬∑ Gwern.net](https://gwern.net/forking-path)

[On Seeing Through and Unseeing: The Hacker Mindset ¬∑ Gwern.net](https://gwern.net/unseeing)

[Slowing Moore‚Äôs Law: How It Could Happen ¬∑ Gwern.net](https://gwern.net/slowing-moores-law)

[The Neural Net Tank Urban Legend ¬∑ Gwern.net](https://gwern.net/tank)

[Evolution as Backstop for Reinforcement Learning ¬∑ Gwern.net](https://gwern.net/backstop)

[Fake Journal Club: Teaching Critical Reading ¬∑ Gwern.net](https://gwern.net/fake-journal-club)

[Why Do Hipsters Steal Stuff? ¬∑ Gwern.net](https://gwern.net/larping)

[Machine Learning Scaling ¬∑ Gwern.net](https://gwern.net/note/scaling)

[The Scaling Hypothesis ¬∑ Gwern.net](https://gwern.net/scaling-hypothesis)

[GPT-3 Nonfiction ¬∑ Gwern.net](https://gwern.net/gpt-3-nonfiction)

[GPT-3 Creative Fiction ¬∑ Gwern.net](https://gwern.net/gpt-3#prompts-as-programming)

[40a93946b61c16a861bb5d277c89bdf07c507d09.pdf](https://gwern.net/doc/www/arxiv.org/40a93946b61c16a861bb5d277c89bdf07c507d09.pdf)

[[1806.11146] Adversarial Reprogramming of Neural Networks](https://ar5iv.labs.arxiv.org/html/1806.11146?fallback=original)

[080e52b3e827dd0c10a822c22935f62305ee1b8f.pdf](https://gwern.net/doc/www/arxiv.org/080e52b3e827dd0c10a822c22935f62305ee1b8f.pdf)

[[1809.01829] Adversarial Reprogramming of Text Classification Neural Networks](https://ar5iv.labs.arxiv.org/html/1809.01829?fallback=original)

[Magna Alta Doctrina - LessWrong](https://www.lesswrong.com/posts/iNaLHBaqh3mL45aH8/magna-alta-doctrina)

[The Brain as a Universal Learning Machine - LessWrong](https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine)

[Bing Chat is blatantly, aggressively misaligned - LessWrong](https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned?commentId=AAC8jKeDp6xqsZK2K)

[Moore's Law, AI, and the pace of progress - LessWrong](https://www.lesswrong.com/posts/aNAFrGbzXddQBMDqh/moore-s-law-ai-and-the-pace-of-progress)

[Proposal: Scaling laws for RL generalization - LessWrong](https://www.lesswrong.com/posts/65qmEJHDw3vw69tKm/proposal-scaling-laws-for-rl-generalization?commentId=bdzbeD9YvarEEopCq)

[Raising the Sanity Waterline - LessWrong](https://www.lesswrong.com/posts/XqmjdBKa4ZaXJtNmf/raising-the-sanity-waterline)

[The Brain as a Universal Learning Machine - LessWrong](https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine)

[Matt Botvinick on the spontaneous emergence of learning algorithms - LessWrong](https://www.lesswrong.com/posts/Wnqua6eQkewL3bqsF/matt-botvinick-on-the-spontaneous-emergence-of-learning)

[Taboo Your Words - LessWrong](https://www.lesswrong.com/posts/WBdvyyHLdxZSAMmoz/taboo-your-words)

[Truthful and honest AI - LessWrong](https://www.lesswrong.com/posts/sdxZdGFtAwHGFGKhg/truthful-and-honest-ai)

[But is it really in Rome? An investigation of the ROME model editing technique - LessWrong](https://www.lesswrong.com/posts/QL7J9wmS6W2fWpofd/but-is-it-really-in-rome-an-investigation-of-the-rome-model)

[A Mechanistic Interpretability Analysis of Grokking - LessWrong](https://www.lesswrong.com/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking#Alignment_Relevance)

[Critique of some recent philosophy of LLMs‚Äô minds - LessWrong](https://www.lesswrong.com/posts/ejEgaYSaefCevapPa/critique-of-some-recent-philosophy-of-llms-minds#Reductionism_and_the_ladder_of_language_model_understanding)

[Simulators - LessWrong](https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators)

[An Equilibrium of No Free Energy - LessWrong](https://www.lesswrong.com/posts/yPLr2tnXbiFXkMWvk/an-equilibrium-of-no-free-energy)

[MIRI announces new "Death With Dignity" strategy - LessWrong](https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy)

[Optimal Employment - LessWrong](https://www.lesswrong.com/posts/jtedBLdducritm8y6/optimal-employment)

[Orthogonality Thesis - Arbital](https://arbital.com/p/orthogonality/)

[Instrumental convergence - Arbital](https://arbital.com/p/instrumental_convergence/)

[Let's See You Write That Corrigibility Tag - AI Alignment Forum](https://www.alignmentforum.org/posts/AqsjZwxHNqH64C2b6/let-s-see-you-write-that-corrigibility-tag)

[AGI Ruin: A List of Lethalities - AI Alignment Forum](https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities)

[Where I agree and disagree with Eliezer - AI Alignment Forum](https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer)

[Some of my disagreements with List of Lethalities - AI Alignment Forum](https://www.alignmentforum.org/posts/kpFxkXBbpF5pWDRrc/some-of-my-disagreements-with-list-of-lethalities)

[‚ÄúPivotal Act‚Äù Intentions: Negative Consequences and Fallacious Arguments - AI Alignment Forum](https://www.alignmentforum.org/posts/Jo89KvfAs9z7owoZp/pivotal-act-intentions-negative-consequences-and-fallacious)

[(My understanding of) What Everyone in Technical Alignment is Doing and Why - AI Alignment Forum](https://www.alignmentforum.org/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is)

[ARC's first technical report: Eliciting Latent Knowledge - AI Alignment Forum](https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge)

[[AN #81]: Universality as a potential solution to conceptual difficulties in intent alignment - AI Alignment Forum](https://www.alignmentforum.org/posts/3kzFPA5uuaGZWg4PS/an-81-universality-as-a-potential-solution-to-conceptual)

[Mundane solutions to exotic problems - AI Alignment Forum](https://www.alignmentforum.org/posts/d5m3G3ov5phZu7FX3/mundane-solutions-to-exotic-problems)

[Optimization daemons - Arbital](https://arbital.com/p/daemons/)

[Clarifying ‚ÄúAI alignment‚Äù. Clarifying what I mean when I say that‚Ä¶ | by Paul Christiano | AI Alignment](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6)

[Oversight Misses 100% of Thoughts The AI Does Not Think - AI Alignment Forum](https://www.alignmentforum.org/posts/98c5WMDb3iKdzD4tM/oversight-misses-100-of-thoughts-the-ai-does-not-think)

[The Main Sources of AI Risk? - AI Alignment Forum](https://www.alignmentforum.org/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk)

[Distinguishing AI takeover scenarios - AI Alignment Forum](https://www.alignmentforum.org/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios)

[My Overview of the AI Alignment Landscape: Threat Models - AI Alignment Forum](https://www.alignmentforum.org/posts/3DFBbPFZyscrAiTKS/my-overview-of-the-ai-alignment-landscape-threat-models)

[What does it take to defend the world against out-of-control AGIs? - AI Alignment Forum](https://www.alignmentforum.org/posts/LFNXiQuGrar3duBzJ/what-does-it-take-to-defend-the-world-against-out-of-control)

[Conjecture Home](https://conjecture.dev/)

[Palimpsest - Wikipedia](https://en.wikipedia.org/wiki/Palimpsest?useskin=vector)

[AI Alignment](https://ai-alignment.com/)

[My research methodology - AI Alignment Forum](https://www.alignmentforum.org/posts/EF5M6CmKRd6qZk27Z/my-research-methodology)

[Testing The Natural Abstraction Hypothesis: Project Update - AI Alignment Forum](https://www.alignmentforum.org/posts/dNzhdiFE398KcGDc9/testing-the-natural-abstraction-hypothesis-project-update)

[Basic Foundations for Agent Models - AI Alignment Forum](https://www.alignmentforum.org/s/ogntdnjG6Y9tbLsNS)

[Gears Which Turn The World - AI Alignment Forum](https://www.alignmentforum.org/s/xEFeCwk3pdYdeG2rL)

[Cartesian Frames - AI Alignment Forum](https://www.alignmentforum.org/s/2A7rrZ4ySx6R8mfoT)

[Finite Factored Sets - AI Alignment Forum](https://www.alignmentforum.org/s/kxs3eeEti9ouwWFzr)

[The ground of optimization - AI Alignment Forum](https://www.alignmentforum.org/posts/znfkdCoHMANwqc2WE/the-ground-of-optimization-1)

[evhub - AI Alignment Forum](https://www.alignmentforum.org/users/evhub)

[Stuart_Armstrong - AI Alignment Forum](https://www.alignmentforum.org/users/stuart_armstrong)

[Intro to Brain-Like-AGI Safety - AI Alignment Forum](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8)

[Epistemic Cookbook for Alignment - AI Alignment Forum](https://www.alignmentforum.org/s/LLEJJoaYpCoS5JYSY)

[Productive Mistakes, Not Perfect Answers - AI Alignment Forum](https://www.alignmentforum.org/posts/ADMWDDKGgivgghxWf/productive-mistakes-not-perfect-answers)

[Epistemological Vigilance for Alignment - AI Alignment Forum](https://www.alignmentforum.org/posts/72scWeZRta2ApsKja/epistemological-vigilance-for-alignment)

[Why Agent Foundations? An Overly Abstract Explanation - AI Alignment Forum](https://www.alignmentforum.org/posts/FWvzwCDRgcjb9sigb/why-agent-foundations-an-overly-abstract-explanation)

[A central AI alignment problem: capabilities generalization, and the sharp left turn - AI Alignment Forum](https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization)

[Refining the Sharp Left Turn threat model, part 1: claims and mechanisms - AI Alignment Forum](https://www.alignmentforum.org/posts/usKXS5jGDzjwqv3FJ/refining-the-sharp-left-turn-threat-model)

[Refining the Sharp Left Turn threat model, part 2: applying alignment techniques - AI Alignment Forum](https://www.alignmentforum.org/s/4iEpGXbD3tQW5atab/p/dfXwJh4X5aAcS8gF5#Step_2__The_goal_aligned_model_preserves_its_goals_during_SLT__with_some_help_from_us_)

[Paradigms of AI alignment: components and enablers | Victoria Krakovna](https://vkrakovna.wordpress.com/2022/06/02/paradigms-of-ai-alignment-components-and-enablers/)

[Our approach to alignment research](https://openai.com/blog/our-approach-to-alignment-research)

[The case for how and why AI might kill us all](https://newatlas.com/technology/ai-danger-kill-everyone/)

[Search - LessWrong](https://www.lesswrong.com/search?query=gpt-n)

[How I'm thinking about GPT-N - LessWrong](https://www.lesswrong.com/posts/iQabBACQwbWyHFKZq/how-i-m-thinking-about-gpt-n)

[[2107.14795] Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795)

[[2008.02217] Hopfield Networks is All You Need](https://arxiv.org/abs/2008.02217)

[(93) Yann LeCun | May 18, 2021 | The Energy-Based Learning Model - YouTube](https://www.youtube.com/watch?v=4lthJd3DNTM)

[[1905.10985] AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence](https://arxiv.org/abs/1905.10985)

[[2105.08050] Pay Attention to MLPs](https://arxiv.org/abs/2105.08050)

[Patches Are All You Need? | OpenReview](https://openreview.net/forum?id=TVHS5Y4dNvM)

[[2110.00476] ResNet strikes back: An improved training procedure in timm](https://arxiv.org/abs/2110.00476)

[The academic contribution to AI safety seems large - EA Forum](https://forum.effectivealtruism.org/posts/8ErtxW7FRPGMtDqJy/the-academic-contribution-to-ai-safety-seems-large)

[GPT-3: a disappointing paper - LessWrong](https://www.lesswrong.com/posts/ZHrpjDc3CepSeeBuE/gpt-3-a-disappointing-paper)

[interpreting GPT: the logit lens - LessWrong](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)

[larger language models may disappoint you [or, an eternally unfinished draft] - LessWrong](https://www.lesswrong.com/posts/pv7Qpu8WSge8NRbpB/larger-language-models-may-disappoint-you-or-an-eternally)

[(1) Home / Twitter](https://twitter.com/home)

[Erik Hanchett on Twitter: "UnoCSS is it worth replacing Tailwind in my next project? üëá https://t.co/TJX1grdtW2" / Twitter](https://twitter.com/ErikCH/status/1642915038481756161)

[(1) Alejandro Piad Morffis on Twitter: "Ok guys, please listen. LLMs have no memory, no recall of past events, no mutable internal state. They are complicated functions that map input strings to output strings. They are incredible nonetheless, no need to overcomplicate things. And no, humans are not "maybe also that"." / Twitter](https://twitter.com/alepiad/status/1642980030799118336)

[(1) john stuart chill on Twitter: "eliezer: AI risk is real ok but he doesn‚Äôt have a degree stephen hawking: AI risk is real ok but not a computer scientist stuart russell: AI risk is real ok he hasn‚Äôt won awards geoffrey hinton: AI risk is real ok but he didn‚Äôt invent cs the ghost of alan turing: AI ris‚Äî" / Twitter](https://twitter.com/mealreplacer/status/1642831621865832448)

[(1) Manuela Malasa√±a on Twitter: "Q: what is a shader? A: a kind of instructions we can give the computer to tell it what to make something look like and now, needlessly complicated ‚Äúshaders for beginners‚Äù, a thread" / Twitter](https://twitter.com/ManuelaXibanya/status/1642277656816123906)

[‚ÄúNon-Player Character‚Äù ‚Äì Eliezer S. Yudkowsky](https://www.yudkowsky.net/other/fiction/npc)

[A Conversation](http://frombob.to/you/aconvers.html)

[Noosphere - Wikipedia](https://en.wikipedia.org/wiki/Noosphere?useskin=vector)

[Stuart J. Russell - Wikipedia](https://en.wikipedia.org/wiki/Stuart_J._Russell?useskin=vector)

[Terrence Deacon - Wikipedia](https://en.wikipedia.org/wiki/Terrence_Deacon?useskin=vector)

[(1) Rob Bensinger üîç on Twitter: "I've been citing https://t.co/jVrdg2mIgz to explain why the situation with AI looks doomy to me. But that post is relatively long, and emphasizes specific open technical problems over "the basics". Here are 10 things I'd focus on if I were giving "the basics" on why I'm worried:" / Twitter](https://twitter.com/robbensinger/status/1643342330290913280)

[(1) Rob Bensinger üîç on Twitter: "@moskov @adamdangelo @ESYudkowsky @ylecun To properly answer your question, @moskov (and @jasoncrawford): I think Eliezer's best write-up on "the basics" is https://t.co/pJjocKqHPQ. Here's my own stab at listing out ten relatively important things behind my high p(doom): https://t.co/00My5hXQBR." / Twitter](https://twitter.com/robbensinger/status/1643388581417992193)

[Gogolian/open-humanity: An Open Source Project that will, gather consensual info from people about traits of their characters, views and beliefs, to fund a database, that can be used in the future to provide those people, or their descendants with chatbots as digital twins of theese people. Saving humanity before they disappear.](https://github.com/Gogolian/open-humanity)

[Train ChatGPT on Your Data - AlphaVenture Experiments](https://ml.alphaventure.com/)

[Discussion with Nate Soares on a key alignment difficulty - LessWrong](https://www.lesswrong.com/posts/iy2o4nQj9DnQD7Yhj/discussion-with-nate-soares-on-a-key-alignment-difficulty)

["Carefully Bootstrapped Alignment" is organizationally hard - LessWrong](https://www.lesswrong.com/posts/thkAtqoQwN6DtaiGT/carefully-bootstrapped-alignment-is-organizationally-hard)

[On AutoGPT - LessWrong](https://www.lesswrong.com/posts/566kBoPi76t8KAkoD/on-autogpt)

[GPTs are Predictors, not Imitators - LessWrong](https://www.lesswrong.com/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators)

[Evolution provides no evidence for the sharp left turn - LessWrong](https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn)

[Scaffolded LLMs as natural language computers - LessWrong](https://www.lesswrong.com/posts/43C3igfmMrE9Qoyfe/scaffolded-llms-as-natural-language-computers)

[Four mindset disagreements behind existential risk disagreements in ML - LessWrong](https://www.lesswrong.com/posts/84BJopKvQ8qYGHY6b/four-mindset-disagreements-behind-existential-risk)

[Killing Socrates - LessWrong](https://www.lesswrong.com/posts/JcgtKunqmELefxksx/killing-socrates)
